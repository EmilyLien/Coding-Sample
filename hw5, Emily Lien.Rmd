**SYS 6018 | Fall 2020 | University of Virginia **

**Emily Lien, egl6a**

General discussion with: Kip McCharen, Clair McLafferty

*******************************************

<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->

```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=FALSE,    # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 5,     # set figure width
                      fig.align = "center",# set figure position
                      out.width = "49%", # set width of displayed images
                      warning=TRUE,      # show R warnings
                      message=TRUE)      # show R messages
options(dplyr.summarise.inform = FALSE)  # ignore message about group structure
```

<!--- Solution Region --->
```{css solution-region, echo=FALSE}
.solution {
  background-color: #232D4B10;
  border-style: solid;
  border-color: #232D4B;
  padding: .5em;
  margin: 20px
}
```


<!--- Load Required R packages here --->
```{r packages, include=FALSE}
#- Better table printing
library(kableExtra) # https://haozhu233.github.io/kableExtra/awesome_table_in_html.html
format_table <- function(x, nmax=10) {
  kable(x) %>% 
    kable_styling(full_width = FALSE, font_size=11, position = "left") %>% 
    {if(nrow(x) > nmax) scroll_box(., width = "100%", height = "200px") else .}
}
#- useful functions
digits <- function(x, k=2) format(round(x, k), nsmall=k)
#- data directory
data.dir = 'https://mdporter.github.io/SYS6018/data/'
#- required functions
library(fitdistrplus)
library(tidyverse)
library(ks)
```




### Problem 5.1 Geographic Profiling

```{r, echo=FALSE, eval=FALSE}
set.seed(2019)
n = 283
sd = 2.1
x = sqrt(rnorm(n, sd=sd)^2 + rnorm(n, sd=sd)^2)

readr::write_csv(tibble(x), "../data/geo_profile.csv", col_names=FALSE)
#hist(x, 15)

```

Geographic profiling, a method developed in criminology, can be used to estimate the [home location (roost) of animals](https://www.sciencedirect.com/science/article/pii/S0022519305004157) based on a collection of sightings. The approach requires an estimate of the distribution the animal will travel from their roost to forage for food. 

A sample of $283$ distances that pipistrelle bats traveled (in meters) from their roost can be found at: 
<https://mdporter.github.io/SYS6018/data/geo_profile.csv>


One probability model for the distance these bats will travel is:
\begin{align*}
f(x; \theta) = \frac{x}{\theta} \exp \left( - \frac{x^2}{2 \theta} \right)
\end{align*}
where the parameter $\theta > 0$ controls how far they are willing to travel. 


a. Derive the MLE for $\theta$ (i.e., show the math). 

<div class="solution"> 
With the understanding that we have been treating log to mean ln, based on the notes:
$L(\theta) = \Sigma_{i=1}^n log(\frac{x_i}{\theta}e^{\frac{-x_i^2}{2\theta}})$

$L(\theta) = \Sigma_{i=1}^n (log(x_i) - log(\theta) - \frac{x_i^2}{2\theta})$

$\frac{dL(\theta)}{d\theta} = \Sigma_{i=1}^n \frac{-1}{\theta} + \frac{x_i^2}{2\theta^2}$

$\frac{dL(\theta)}{d\theta} = \frac{-n}{\theta} +\frac{1}{2\theta^2}\Sigma_{i=1}^n x_i^2$

Set the derivative equal to 0:
$\frac{-n}{\theta} +\frac{1}{2\theta^2}\Sigma_{i=1}^n x_i^2 = 0$

Multiply both sides by $\theta^2$

$-n\theta + \frac{\Sigma_{i=1}^n x_i^2}{2} = 0$

Isolate $\theta$

$\theta = \frac{\Sigma_{i=1}^n x_i^2}{2n}$
</div>



b. What is the MLE of $\theta$ for the bat data? (Use results from a, or use computational methods.) 

<div class="solution"> 
```{r}
#Implementing result from part a
Bats<-read.csv('geo_profile.csv',header=FALSE)
BatSq<-Bats^2
BatSqSum<-sum(BatSq)

#Final theta value that maximizes the likelihood
theta<-BatSqSum/(2*283)
theta
```

</div>



c. Using the MLE value of $\theta$ from part b, compute the estimated density of this distribution at a set of evaluation points between 0 and 8 meters. Plot the estimated density.

<div class="solution"> 
```{r}
#Taking a look at the initial shape
ggplot(Bats) + geom_histogram(aes(x=V1),fill="darkorchid4")

#Getting estimations using the pdf and theta calculated above
pdf<-function(x) (x/theta)*exp(-(x)^2/(theta*2))
loglike<-pdf(Bats$V1)

Bats2<-tibble('Miles'=Bats$V1,'Probs'=loglike)

#Histogram help from density.R
ggplot(Bats2) +
  geom_histogram(aes(x=Miles, y=after_stat(density)),fill="darkorchid4")+
  geom_line(aes(Miles, Probs),col="green2")
+labs(x = expression(Miles))
```

</div>



d. Estimate the density using KDE. Report the bandwidth you chose and produce a plot of the estimated density. 


<div class="solution"> 
```{r}
Bats.kde<-kde(Bats2$Miles)
plot(Bats.kde)

#After plotting some alternatives, I decided I liked the default h value of 0.3910206

KDE<-tibble(x=Bats.kde$eval.points,y=Bats.kde$estimate)

ggplot(Bats2) +
  geom_histogram(aes(x=Miles, y=after_stat(density)),fill="darkorchid4")+
  geom_function(fun=pdf,col="green")+
  geom_line(data=KDE,aes(x,y),col="cyan")
+labs(x = expression(Miles))
```
</div>



e. Which model do you prefer, the parametric or KDE? 

<div class="solution"> 
Hard to say. They both look good against the data. I enjoyed doing the math behind the parametric estimation, and like knowing what functions I'm working with. However, the KDE was very quick to implement, and it represents the peak in the data better. I think I like them both, it just depends on situational things such as how exact do I need my estimate, do I have a known pdf I can work with, how fast does this need to be done, etc.
</div>





### Problem 5.2: Interstate Crash Density

Interstate 64 (I-64) is a major east-west road that passes just south of Charlottesville. Where and when are the most dangerous places/times to be on I-64? The crash data (link below) gives the mile marker and fractional time-of-week for crashes that occurred on I-64 between mile marker 87 and 136 in 2016. The time-of-week data takes a numeric value of *\<dow\>.\<hour/24\>*, where the dow starts at 0 for Sunday (6 for Sat) and the decimal gives the time of day information. Thus `time=0.0417` corresponds to Sun at 1am and `time=6.5` corresponds to Sat at noon). 

- **Crash Data**: <https://mdporter.github.io/SYS6018/data/crashes16.csv>


a. Extract the crashes and make a scatter plot with mile marker on x-axis and time on y-axis. 


<div class="solution"> 
```{r}
crash<-read.csv('crashes16.csv',header=TRUE,sep=',')
ggplot(crash) + geom_point(aes(x=mile, y=time))
```
</div>



b. Use KDE to estimate the *mile marker* density. Report the bandwidth and plot the density estimate. 

<div class="solution"> 
```{r}
#After testing a few different values of h, I decided I liked h=2 because I can see a little more detail without getting too spiky 
miles.kde<-kde(crash$mile,h=2)
plot(miles.kde,xlab='mile marker')
```
</div>


c. Use KDE to estimate the temporal *time-of-week* density. Report the bandwidth and plot the density estimate. 

<div class="solution"> 
```{r}
#For this one, I chose an h value of .3, because I was interested in seeing more of the peaks for when crashes were happening
time.kde<-kde(crash$time,h=.3)
plot(time.kde,xlab='time')
```
</div>



d. Use KDE to estimate the bivariate mile-time density. What are the bandwidth parameters? Plot the bivariate density estimate. 


<div class="solution"> 
```{r}
H1<-Hscv(crash)
crash.kde<-kde(crash,H=H1)
plot(crash.kde)
```
With variance-covariance matrix:

           [,1]       [,2]
           
[1,] 25.9921201 -0.1523207

[2,] -0.1523207  0.3660496
</div>


e. Based on the estimated density, approximate the most dangerous mile marker and time-of-week. 

<div class="solution"> 
Looks like around mile marker 119, with time code 1.5 which translates to Monday at noon. Lunch rush around Exit 118, anybody?
</div>

